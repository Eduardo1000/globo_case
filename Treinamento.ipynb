{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import tempfile\n",
    "import mlflow\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "matplotlib.style.use('ggplot')\n",
    "pd.set_option('max_rows', 500)\n",
    "pd.set_option('max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bd_dem_time(_df, args):\n",
    "    \n",
    "    df_bem_time = _df.copy()\n",
    "    # Processa coluna de nascimento\n",
    "    column = 'DTA_NASC'\n",
    "    mask = (df_bem_time[column]>args['ano_antigo']) & (df_bem_time[column]<args['ano_atual'])\n",
    "    df_bem_time.loc[~mask, column] = None # joga quem ta fora da data para None\n",
    "    df_bem_time['IDADE'] = args['ano_atual'] - df_bem_time[column]\n",
    "    df_bem_time = df_bem_time.drop(columns=[column])\n",
    "\n",
    "    # Processa coluna de sexo\n",
    "    column='SEXO'\n",
    "    mask = (df_bem_time[column] == 'M') | (df_bem_time[column] == 'F')\n",
    "    df_bem_time.loc[~mask, 'SEXO'] = None\n",
    "    \n",
    "    dummies = pd.get_dummies(df_bem_time[column]).astype('category')\n",
    "    df_bem_time = pd.concat([df_bem_time, dummies], axis=1).drop(columns=[column])\n",
    "\n",
    "    # Processa coluna de estado\n",
    "    column = 'ESTADO'\n",
    "    df_bem_time.loc[df_bem_time[column]=='Nao informado', column] = None\n",
    "    \n",
    "    dummies = pd.get_dummies(df_bem_time[column]).astype('category')\n",
    "    df_bem_time = pd.concat([df_bem_time, dummies], axis=1).drop(columns=[column])\n",
    "\n",
    "    # Processa coluna de time\n",
    "    column = 'TIME'\n",
    "    y_categories = df_bem_time[column].astype('category').cat.categories\n",
    "    df_bem_time[column] = df_bem_time[column].astype('category').cat.codes\n",
    "    \n",
    "    return df_bem_time, y_categories\n",
    "\n",
    "def transform_idade(df, args):\n",
    "    _df = df.copy()\n",
    "    \n",
    "    # Preprocessa coluna de idade\n",
    "    column='IDADE'\n",
    "    df_cut, bin_names = pd.cut(_df[column], bins=args['idade_bins'], retbins=True)\n",
    "    bin_names = bin_names.astype('int').astype('str')\n",
    "    bin_names = [f'{bin_names[i]}_{bin_names[i+1]}' for i in range(args['idade_bins'])]\n",
    "    dummies = pd.get_dummies(df_cut).astype('category')\n",
    "    dummies.columns = [f'{column}_' +x for x in bin_names] \n",
    "    _df = pd.concat([_df, dummies], axis=1).drop(columns=[column])\n",
    "    return _df\n",
    "\n",
    "def process_servicos(_df, args):\n",
    "    df_servicos = _df.copy().astype('category')\n",
    "    return df_servicos\n",
    "\n",
    "def clean_url(_df, args, y_categories):\n",
    "    print('Limpando URL')\n",
    "    df_consumo = _df.copy()\n",
    "    \n",
    "    # remove o nome do site, pega apenas os argumentos\n",
    "    df_consumo['url'] = df_consumo['url'].str.split(\".com/\").str.get(1)\n",
    "    \n",
    "    # Split da url em futebol\n",
    "    split_futebol = df_consumo['url'].str.split(\"futebol/\")\n",
    "\n",
    "    # Define o url_region\n",
    "    column = 'url_region'\n",
    "    if args['region'][f'{column}_bool']:\n",
    "        df_consumo[column] = split_futebol.str.get(0).str[:-1]\n",
    "        df_consumo[column] = df_consumo[column].str.split(\"/\").str.get(0)\n",
    "        df_consumo[column] = df_consumo[column].str.lower()\n",
    "        df_consumo.loc[df_consumo[column].str.len() > 2, column] = None\n",
    "        df_consumo.loc[df_consumo[column] == '', column] = None\n",
    "\n",
    "    # Split da url em noticia\n",
    "    split_noticia = split_futebol.str.get(1).str.split(\"noticia/\")\n",
    "\n",
    "    # Define o url_noticia\n",
    "    column = 'url_noticia'\n",
    "    if args['noticia'][f'{column}_bool']:\n",
    "        df_consumo['url_noticia'] = split_noticia.str.get(1)\n",
    "\n",
    "    # Define o url_campeonato\n",
    "    column = 'url_campeonato'\n",
    "    if args['campeonato'][f'{column}_bool']:\n",
    "        df_consumo[column] = split_noticia.str.get(0).str.split(\"times/\").str.get(0).str[:-1]\n",
    "        df_consumo[column] = df_consumo[column].str.split(\"/\").str.get(0)\n",
    "        df_consumo.loc[df_consumo[column] == '', column] = None\n",
    "        df_consumo[column] = df_consumo[column].str.replace('-', '')\n",
    "\n",
    "    # Define o url_times\n",
    "    column = 'url_times'\n",
    "    if args['times'][f'{column}_bool']:\n",
    "        df_consumo[column] = split_noticia.str.get(0).str.split(\"times/\").str.get(1).str[:-1]\n",
    "        df_consumo[column] = df_consumo[column].str.split(\"/\").str.get(0)\n",
    "        df_consumo.loc[df_consumo[column] == '', column] = None\n",
    "        # retira times que não esteja nos times da variável alvo\n",
    "        df_consumo[column] = df_consumo[column].str.lower().str.replace('-', '').str.replace(' ', '').str.replace('siga', '')\n",
    "        nome_times = y_categories.str.replace('-', '').str.replace(' ', '').str.lower().unique()\n",
    "        df_consumo.loc[df_consumo[column].isin(nome_times)==False, column] = None\n",
    "\n",
    "    # Remove a coluna Url original\n",
    "    df_consumo.drop(columns=['url'], inplace=True)\n",
    "    return df_consumo\n",
    "    \n",
    "def handle_dummies_column(df_consumo, args, column, drop=True):\n",
    "    print('Categorizando coluna de ', column)\n",
    "    dummies = pd.get_dummies(df_consumo[column]).astype('float32')\n",
    "    dummies.columns = f'{column}_'+dummies.columns\n",
    "    if args[f'{column}_count']:\n",
    "        df_consumo = pd.concat([df_consumo, dummies], axis=1)\n",
    "    if args[f'{column}_time']:\n",
    "        time_dummies = dummies.apply(lambda x: x*df_consumo['tempo'])\n",
    "        time_dummies.columns = time_dummies.columns + '_time'\n",
    "        df_consumo = pd.concat([df_consumo, time_dummies], axis=1)\n",
    "    if drop:\n",
    "        df_consumo = df_consumo.drop(columns=[column])\n",
    "    return df_consumo\n",
    "    \n",
    "def process_consumo(_df, args, y_categories):\n",
    "    df_consumo = _df.copy()\n",
    "    df_consumo = df_consumo.dropna()\n",
    "    print('Tamnho do dataset de Consumo', df_consumo.shape)\n",
    "    \n",
    "    # Limpa a Coluna de url, transformando-a em 4 colunas\n",
    "    df_consumo = clean_url(df_consumo, args, y_categories)\n",
    "    \n",
    "    # Trata os dados de Regiao\n",
    "    column = 'url_region'\n",
    "    if args['region'][f'{column}_bool']:\n",
    "        df_consumo = handle_dummies_column(df_consumo, args=args['region'], column=column)\n",
    "    \n",
    "    # Trata os dados de Campeonato\n",
    "    column = 'url_campeonato'\n",
    "    if args['campeonato'][f'{column}_bool']:\n",
    "        df_consumo = handle_dummies_column(df_consumo, args=args['campeonato'], column=column)\n",
    "    \n",
    "    # Trata os dados de Times\n",
    "    column = 'url_times'\n",
    "    if args['times'][f'{column}_bool']:\n",
    "        drop_time=True\n",
    "        if args['noticia'][f'url_noticia_bool']:\n",
    "            drop_time=False\n",
    "        df_consumo = handle_dummies_column(df_consumo, args=args['times'], column=column, drop=drop_time)\n",
    "    \n",
    "    # Trata os dados de Noticia\n",
    "    column = 'url_noticia'\n",
    "    if args['noticia'][f'{column}_bool']:\n",
    "        print('Lendo coluna de  url_times')\n",
    "        url_time_columns = df_consumo.columns[df_consumo.columns.str.startswith('url_times_')]\n",
    "        for column in url_time_columns:\n",
    "            nome_time = column.split('_')[-1]\n",
    "            mask = (df_consumo['url_noticia'].str.contains(nome_time)) & (df_consumo['url_times'].isna())\n",
    "            print('Nome Antes: ', nome_time, df_consumo.loc[mask, column].sum())\n",
    "            df_consumo.loc[mask, column] += 1\n",
    "            print('Nome Depois: ', nome_time, df_consumo.loc[mask, column].sum())\n",
    "\n",
    "        df_consumo.drop(columns=[column, 'url_times'], inplace=True)\n",
    "    \n",
    "    # Agrega o consumo por Usuário\n",
    "    df_consumo = df_consumo.groupby('KEY').sum().reset_index()\n",
    "    \n",
    "    return df_consumo\n",
    "\n",
    "def concat_data(df_bem_time, df_servicos, df_consumo):\n",
    "    return df_bem_time.set_index('KEY').join(df_servicos.set_index('KEY')).join(df_consumo.set_index('KEY')).fillna(0)\n",
    "\n",
    "def split_X_y(df):\n",
    "    target_column = 'TIME'\n",
    "    return df.drop(columns=[target_column]), df[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_args):\n",
    "    if model_args['model_name'] == 'SVM':\n",
    "        model = LinearSVC(C=model_args['C'])\n",
    "    return model\n",
    "\n",
    "def create_heatmap(y_test, y_pred, y_categories, name_file='example.png', normalize=False):\n",
    "    \n",
    "    fig = plt.figure(figsize = (20, 16))\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    df_cm = pd.DataFrame(cnf_matrix, index = y_categories, columns = y_categories)\n",
    "    \n",
    "    annot=False\n",
    "    title='Matriz de Confusão'\n",
    "    if normalize:\n",
    "        df_cm=(df_cm-df_cm.mean())/df_cm.std() #normaliza a matriz\n",
    "        df_cm=df_cm.round(2)\n",
    "        annot=True\n",
    "        title = title + ' Normalizada'\n",
    "    \n",
    "    plt.title(title)\n",
    "    sns_plot = sn.heatmap(df_cm, annot=annot, cmap=\"YlGnBu\", linewidths=.5)\n",
    "            \n",
    "    # Save Figure\n",
    "    dirpath = tempfile.mkdtemp()\n",
    "    save_path = os.path.join(dirpath, name_file)\n",
    "    fig = sns_plot.get_figure()\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "    return save_path\n",
    "\n",
    "def get_metrics(y_test, y_pred):\n",
    "    metrics = {\n",
    "        'acc': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def get_general_metrics(json_metrics, test=False):\n",
    "    df_metrics = pd.DataFrame.from_dict(json_metrics)\n",
    "    df_metrics = df_metrics.mean(axis=1)\n",
    "    df_metrics.index = 'train_'+df_metrics.index\n",
    "    return df_metrics.to_dict()\n",
    "\n",
    "def recursive_log_params(dict_params):\n",
    "    for key, value in dict_params.items():\n",
    "        if isinstance(value, dict):\n",
    "            recursive_log_params(value)\n",
    "        else: \n",
    "            mlflow.log_param(key, value)\n",
    "    return\n",
    "\n",
    "def model_fit(X_train, y_train, model_args):\n",
    "    if model_args['model_name'] == 'SVM':\n",
    "        model = LinearSVC(C=model_args['C'])\n",
    "        model.fit(X_train, y_train)\n",
    "    elif model_args['model_name'] == 'LGBM':\n",
    "        params = model_args.copy()\n",
    "        params.pop('model_name')\n",
    "        d_train=lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "        model=lgb.train(params, d_train)\n",
    "    return model\n",
    "\n",
    "def model_predict(model, X_test, model_args):\n",
    "    y_pred = model.predict(X_test)\n",
    "    if model_args['model_name'] == 'LGBM':\n",
    "        y_pred=[np.argmax(line) for line in y_pred]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametros"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hiperparams = {\n",
    "    \"data\": {\n",
    "        \"bd_dem_time\":{\n",
    "            \"ano_atual\": 2021,\n",
    "            \"ano_antigo\": hp.choice(\"d_ano_antigo\", (1900, 1920))\n",
    "        }\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"lgbm\":{\n",
    "            \n",
    "        },\n",
    "        \"svm_linear\": {\n",
    "            \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparams = {\n",
    "    \"data\": {\n",
    "        \"test_split\":0.2,\n",
    "        \"n_folds\": 5, # numero de folds\n",
    "        \"smote\": True, \n",
    "        \"pca\": 1,\n",
    "        \"bd_dem_time\":{\n",
    "            \"ano_atual\": 2021,\n",
    "            \"ano_antigo\": 1920,\n",
    "            \"idade_bins\": 20\n",
    "        },\n",
    "        \"servicos\": {\n",
    "            \n",
    "        },\n",
    "        \"consumo\": {\n",
    "            \"nrows\": 1500000000,  # numero de linha a ler na tabela de consumo, usar um big M para ler tudo\n",
    "            # Regiao não terá time ==True\n",
    "            \"region\":{\n",
    "                \"url_region_bool\": True,\n",
    "                \"url_region_count\": True,\n",
    "                \"url_region_time\": False,\n",
    "            },\n",
    "            \"campeonato\":{\n",
    "                \"url_campeonato_bool\": False,\n",
    "                \"url_campeonato_count\": True,\n",
    "                \"url_campeonato_time\": False,\n",
    "            },\n",
    "            \"times\":{\n",
    "                \"url_times_bool\": True,\n",
    "                \"url_times_count\": True,\n",
    "                \"url_times_time\": False,\n",
    "            },\n",
    "            # Noticia não terá time ==True\n",
    "            \"noticia\":{\n",
    "                \"url_noticia_bool\": False,\n",
    "                \"url_noticia_count\": True,\n",
    "                \"url_noticia_time\": False,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    #\"model\": {\n",
    "    #    \"model_name\": \"SVM\",\n",
    "    #    \"C\": 0.1\n",
    "    #}\n",
    "    \"model\": {\n",
    "        \"model_name\": \"LGBM\",\n",
    "        \"boosting_type\": \"gbdt\", #GradientBoostingDecisionTree\n",
    "        \"objective\": \"multiclass\", #Multi-class target feature\n",
    "        \"metric\": \"multi_logloss\", #metric for multi-class\n",
    "        \"num_class\": 29, #no.of unique values in the target class not inclusive of the end value\n",
    "        #\"force_row_wise\": True\n",
    "        #\"feature_fraction\": 0.8,\n",
    "        \"min_data_in_leaf\": 100,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"max_depth\": 9,\n",
    "        \"num_leaves\": 60,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BD_SERVICOS.csv', 'BD_CONSUMO.csv', 'BD_DEM_TIME.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'dataset'\n",
    "files = [x for x in os.listdir(data_path) if x.endswith('.csv')]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o Arquivo de bd dem time\n",
    "df_bem_time = pd.read_csv(os.path.join(data_path, 'BD_DEM_TIME.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide entre Trein e Teste\n",
    "df_bem_time_train, df_bem_time_test = train_test_split(df_bem_time, test_size=hiperparams['data']['test_split'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o Arquivo de servicos\n",
    "df_servicos = pd.read_csv(os.path.join(data_path, 'BD_SERVICOS.csv'))\n",
    "# Abre o Arquivo de consumo\n",
    "df_consumo = pd.read_csv(os.path.join(data_path, 'BD_CONSUMO.csv'), nrows=hiperparams['data']['consumo']['nrows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bem_time_train_processed, y_categories = process_bd_dem_time(df_bem_time_train, \n",
    "                                                                hiperparams[\"data\"][\"bd_dem_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamnho do dataset de Consumo (14174706, 3)\n",
      "Limpando URL\n",
      "Categorizando coluna de  url_region\n",
      "Categorizando coluna de  url_times\n"
     ]
    }
   ],
   "source": [
    "df_servicos_processed = process_servicos(df_servicos, hiperparams[\"data\"][\"servicos\"])\n",
    "df_consumo_processed = process_consumo(df_consumo, hiperparams[\"data\"][\"consumo\"], y_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = concat_data(df_bem_time_train_processed, df_servicos_processed, df_consumo_processed)\n",
    "X, y = split_X_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN FOLD:  1\n",
      "TRAIN FOLD:  2\n",
      "TRAIN FOLD:  3\n",
      "TRAIN FOLD:  4\n",
      "TRAIN FOLD:  5\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=hiperparams['data']['n_folds'], shuffle=True, random_state=12345)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    recursive_log_params(dict_params=hiperparams)\n",
    "    \n",
    "    metrics = {}\n",
    "    for fold_number, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        step = fold_number+1\n",
    "\n",
    "        print(\"TRAIN FOLD: \", step)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        if hiperparams['data']['pca'] < 1:\n",
    "            pca = PCA(n_components=hiperparams['data']['pca'])\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "\n",
    "        if hiperparams['data']['smote']:\n",
    "            # x percentual de valores da classe mais representativa\n",
    "            sample = y_train.value_counts()\n",
    "            min_desired_data = sample.median()\n",
    "            sample[sample<min_desired_data] = min_desired_data\n",
    "\n",
    "            oversample = SMOTENC(\n",
    "                X_train.dtypes=='category', \n",
    "                sampling_strategy=sample.to_dict(), \n",
    "                random_state=12345, \n",
    "                k_neighbors=5, \n",
    "                n_jobs=-1)\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "            \n",
    "        X_train = transform_idade(X_train, hiperparams[\"data\"][\"bd_dem_time\"])\n",
    "        X_test = transform_idade(X_test, hiperparams[\"data\"][\"bd_dem_time\"])\n",
    "        \n",
    "        continuous_features = X_train.select_dtypes(exclude=['category']).columns\n",
    "        sc=StandardScaler()\n",
    "        X_train[continuous_features]=sc.fit_transform(X_train[continuous_features])\n",
    "        X_test[continuous_features]=sc.transform(X_test[continuous_features])\n",
    "        \n",
    "        model = model_fit(X_train, y_train, model_args=hiperparams['model'])\n",
    "    \n",
    "        # Faz a Predição\n",
    "        y_pred = model_predict(model, X_test, model_args=hiperparams['model'])\n",
    "\n",
    "        fold_metric = get_metrics(y_test, y_pred)\n",
    "        mlflow.log_metrics(fold_metric, step=step)\n",
    "        metrics[step] = fold_metric\n",
    "\n",
    "        # Cria e salva heatmap\n",
    "        save_path = create_heatmap(y_test, y_pred, y_categories, name_file=f'confusion_matrix_fold_{step}.png')\n",
    "        mlflow.log_artifact(save_path)\n",
    "        \n",
    "        # Cria e salva heatmap normalizado\n",
    "        save_path = create_heatmap(y_test, y_pred, y_categories, \n",
    "                                   name_file=f'confusion_matrix_fold_{step}_normalized.png', normalize=True)\n",
    "        mlflow.log_artifact(save_path)\n",
    "\n",
    "    summary_metrics = get_general_metrics(metrics) \n",
    "    mlflow.log_metrics(summary_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout para o teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bem_time_test_processed, y_categories = process_bd_dem_time(df_bem_time_test, \n",
    "                                                                hiperparams[\"data\"][\"bd_dem_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = concat_data(df_bem_time_test_processed, df_servicos_processed, df_consumo_processed)\n",
    "X_final, y_final = split_X_y(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((832062, 114), (190293, 114))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "X_train=pd.DataFrame(sc.fit_transform(X_train))\n",
    "X_final=pd.DataFrame(sc.transform(X_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hiperparams['data']['smote']:\n",
    "    # x percentual de valores da classe mais representativa\n",
    "    sample = y_train.value_counts()\n",
    "    min_desired_data = sample.median()\n",
    "    sample[sample<min_desired_data] = min_desired_data\n",
    "\n",
    "    oversample = SMOTE(sampling_strategy=sample.to_dict(), random_state=12345, k_neighbors=5, n_jobs=-1)\n",
    "    X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_fit(X_train, y_train, model_args=hiperparams['model'])\n",
    "    \n",
    "# Faz a Predição\n",
    "y_pred = model_predict(model, X_final, model_args=hiperparams['model'])\n",
    "\n",
    "fold_metric = get_metrics(y_final, y_pred)\n",
    "\n",
    "# Cria e salva heatmap\n",
    "save_path = create_heatmap(y_final, y_pred, y_categories, name_file=f'confusion_matrix_fold_{step}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = create_heatmap(y_final, y_pred, y_categories, \n",
    "                                   name_file=f'confusion_matrix_fold_{step}_normalized.png', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.6594409673503492, 'f1': 0.658173888096062}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
